{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Assignment - Starter Notebook\n",
    "\n",
    "**Course:** [Course Name]  \n",
    "**Assignment:** Text Classification - Sentiment Analysis  \n",
    "**Team Members:** [Add your team member names here]\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "This notebook implements a sentiment analysis system comparing traditional machine learning and deep learning approaches. We will:\n",
    "\n",
    "1. **Dataset Selection & EDA**: Choose and analyze a publicly available sentiment dataset\n",
    "2. **Preprocessing**: Clean text, tokenize, handle stopwords, and apply embeddings\n",
    "3. **Model Implementation**: Compare traditional ML (Logistic Regression, SVM, Naive Bayes) vs Deep Learning (RNN, LSTM, GRU)\n",
    "4. **Experimentation**: Conduct systematic experiments with different parameters\n",
    "5. **Evaluation**: Compare models using appropriate metrics\n",
    "\n",
    "## Team Member Contributions\n",
    "\n",
    "| Team Member | Contributions |\n",
    "|-------------|---------------|\n",
    "| [Name 1] | [Specific tasks] |\n",
    "| [Name 2] | [Specific tasks] |\n",
    "| [Name 3] | [Specific tasks] |\n",
    "| [Name 4] | [Specific tasks] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text preprocessing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Traditional ML models\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Word embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Selection and Loading\n",
    "\n",
    "Choose one of the following datasets:\n",
    "- **IMDB Movie Reviews**: 50K movie reviews for binary sentiment classification\n",
    "- **Twitter Sentiment Analysis**: Various Twitter datasets\n",
    "- **Amazon Product Reviews**: Product reviews with ratings\n",
    "\n",
    "For this starter, we'll use the IMDB dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load IMDB dataset from TensorFlow/Keras\n",
    "def load_imdb_dataset():\n",
    "    \"\"\"Load IMDB dataset from Keras datasets\"\"\"\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "    \n",
    "    # Load the dataset\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "    \n",
    "    # Get word index for decoding\n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    \n",
    "    # Decode reviews\n",
    "    def decode_review(text):\n",
    "        return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
    "    \n",
    "    # Convert to text format\n",
    "    train_texts = [decode_review(review) for review in x_train]\n",
    "    test_texts = [decode_review(review) for review in x_test]\n",
    "    \n",
    "    # Create DataFrames\n",
    "    train_df = pd.DataFrame({\n",
    "        'text': train_texts,\n",
    "        'sentiment': y_train\n",
    "    })\n",
    "    \n",
    "    test_df = pd.DataFrame({\n",
    "        'text': test_texts,\n",
    "        'sentiment': y_test\n",
    "    })\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Option 2: Load from CSV (if you have downloaded the dataset)\n",
    "def load_csv_dataset(file_path):\n",
    "    \"\"\"Load dataset from CSV file\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Option 3: Load sample dataset for testing\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a small sample dataset for testing\"\"\"\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
    "            \"Terrible movie, waste of time. Very disappointing.\",\n",
    "            \"Great acting and storyline. Highly recommended!\",\n",
    "            \"Boring and predictable. Not worth watching.\",\n",
    "            \"Amazing cinematography and excellent plot development.\",\n",
    "            \"Poor dialogue and weak character development.\",\n",
    "            \"Outstanding performance by the lead actor.\",\n",
    "            \"Confusing plot and bad editing.\",\n",
    "            \"Brilliant direction and beautiful soundtrack.\",\n",
    "            \"Worst movie I have ever seen. Complete disaster.\"\n",
    "        ],\n",
    "        'sentiment': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "    }\n",
    "    return pd.DataFrame(sample_data)\n",
    "\n",
    "# Load your chosen dataset\n",
    "print(\"Choose your dataset loading method:\")\n",
    "print(\"1. IMDB from Keras (recommended)\")\n",
    "print(\"2. Load from CSV file\")\n",
    "print(\"3. Use sample dataset\")\n",
    "\n",
    "# For this example, we'll use the sample dataset\n",
    "# Uncomment the method you want to use:\n",
    "\n",
    "# Method 1: IMDB from Keras\n",
    "# train_df, test_df = load_imdb_dataset()\n",
    "# df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Method 2: From CSV\n",
    "# df = load_csv_dataset('your_dataset.csv')\n",
    "\n",
    "# Method 3: Sample dataset\n",
    "df = create_sample_dataset()\n",
    "\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Understanding the dataset through statistical analysis and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== Dataset Overview ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "print(\"=== Sentiment Distribution ===\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['sentiment'].value_counts(normalize=True))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Sentiment distribution pie chart\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sentiment_counts.values, labels=['Negative', 'Positive'], autopct='%1.1f%%')\n",
    "plt.title('Sentiment Distribution')\n",
    "\n",
    "# Sentiment distribution bar chart\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=df, x='sentiment')\n",
    "plt.title('Sentiment Count')\n",
    "plt.xlabel('Sentiment (0=Negative, 1=Positive)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"=== Text Length Statistics ===\")\n",
    "print(\"Character count statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Text length distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['text_length'], bins=30, alpha=0.7)\n",
    "plt.title('Distribution of Text Length (Characters)')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['word_count'], bins=30, alpha=0.7)\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Text length by sentiment\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=df, x='sentiment', y='word_count')\n",
    "plt.title('Word Count by Sentiment')\n",
    "plt.xlabel('Sentiment (0=Negative, 1=Positive)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "def analyze_word_frequency(texts, title, top_n=20):\n",
    "    \"\"\"Analyze word frequency in texts\"\"\"\n",
    "    # Combine all texts\n",
    "    all_text = ' '.join(texts).lower()\n",
    "    \n",
    "    # Simple tokenization and word counting\n",
    "    words = re.findall(r'\\b\\w+\\b', all_text)\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Get most common words\n",
    "    common_words = word_freq.most_common(top_n)\n",
    "    \n",
    "    return common_words\n",
    "\n",
    "# Analyze word frequency for each sentiment\n",
    "positive_texts = df[df['sentiment'] == 1]['text'].tolist()\n",
    "negative_texts = df[df['sentiment'] == 0]['text'].tolist()\n",
    "all_texts = df['text'].tolist()\n",
    "\n",
    "print(\"=== Word Frequency Analysis ===\")\n",
    "\n",
    "# Overall word frequency\n",
    "overall_freq = analyze_word_frequency(all_texts, \"Overall\")\n",
    "print(\"Most common words overall:\")\n",
    "for word, count in overall_freq[:10]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Positive sentiment words\n",
    "positive_freq = analyze_word_frequency(positive_texts, \"Positive\")\n",
    "print(\"\\nMost common words in positive reviews:\")\n",
    "for word, count in positive_freq[:10]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Negative sentiment words\n",
    "negative_freq = analyze_word_frequency(negative_texts, \"Negative\")\n",
    "print(\"\\nMost common words in negative reviews:\")\n",
    "for word, count in negative_freq[:10]:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "Essential preprocessing steps for effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase=True,\n",
    "                 remove_punctuation=True,\n",
    "                 remove_stopwords=True,\n",
    "                 stemming=False,\n",
    "                 lemmatization=True,\n",
    "                 remove_numbers=True):\n",
    "        \n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stemming = stemming\n",
    "        self.lemmatization = lemmatization\n",
    "        self.remove_numbers = remove_numbers\n",
    "        \n",
    "        # Initialize NLTK tools\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean individual text\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string\n",
    "        text = str(text)\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove numbers\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        # Stemming\n",
    "        if self.stemming:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # Lemmatization\n",
    "        if self.lemmatization:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Remove empty tokens\n",
    "        tokens = [token for token in tokens if token.strip()]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess_texts(self, texts):\n",
    "        \"\"\"Preprocess list of texts\"\"\"\n",
    "        return [self.clean_text(text) for text in texts]\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    stemming=False,\n",
    "    lemmatization=True,\n",
    "    remove_numbers=True\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying text preprocessing...\")\n",
    "df['cleaned_text'] = df['text'].apply(preprocessor.clean_text)\n",
    "\n",
    "# Show before and after examples\n",
    "print(\"\\n=== Preprocessing Examples ===\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {df.iloc[i]['text']}\")\n",
    "    print(f\"Cleaned:  {df.iloc[i]['cleaned_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Creating different text representations for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training set sentiment distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test set sentiment distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TF-IDF Vectorization\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),  # Include both unigrams and bigrams\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# 2. Count Vectorization (Bag of Words)\n",
    "print(\"\\nCreating Count (BoW) features...\")\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Count feature matrix shape: {X_train_count.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Word2Vec Embeddings\n",
    "print(\"Creating Word2Vec embeddings...\")\n",
    "\n",
    "# Prepare tokenized sentences for Word2Vec\n",
    "tokenized_sentences = [text.split() for text in X_train]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def get_word2vec_embeddings(texts, model, vector_size=100):\n",
    "    \"\"\"Convert texts to Word2Vec embeddings by averaging word vectors\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in model.wv:\n",
    "                word_vectors.append(model.wv[word])\n",
    "        \n",
    "        if word_vectors:\n",
    "            # Average the word vectors\n",
    "            text_vector = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            # If no words found, use zero vector\n",
    "            text_vector = np.zeros(vector_size)\n",
    "        \n",
    "        embeddings.append(text_vector)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_train_w2v = get_word2vec_embeddings(X_train, w2v_model)\n",
    "X_test_w2v = get_word2vec_embeddings(X_test, w2v_model)\n",
    "\n",
    "print(f\"Word2Vec embeddings shape: {X_train_w2v.shape}\")\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Traditional Machine Learning Models\n",
    "\n",
    "Implementing and comparing traditional ML approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional ML models dictionary\n",
    "traditional_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Feature sets to test\n",
    "feature_sets = {\n",
    "    'TF-IDF': (X_train_tfidf, X_test_tfidf),\n",
    "    'Count': (X_train_count, X_test_count),\n",
    "    'Word2Vec': (X_train_w2v, X_test_w2v)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "traditional_results = []\n",
    "\n",
    "print(\"Training traditional ML models...\\n\")\n",
    "\n",
    "for feature_name, (X_tr, X_te) in feature_sets.items():\n",
    "    print(f\"=== Using {feature_name} features ===\")\n",
    "    \n",
    "    for model_name, model in traditional_models.items():\n",
    "        print(f\"Training